---
layout: post
title: Foolproof A/B testing performance sorting

excerpt: Frequentist hates him, find out why.
---


Imagine you have a list of experiments that you need to compare with each other. Your data look like this 



| Trial | Success | Failure |     |   |
|------:|--------:|--------:|-----|---|
|   100 |      40 |      60 |     |   |
|    10 |       4 |       6 |     |   |
| ...   | ...     | ...     | ... |   |



Here you have a list of experiments, each is a pair of something like (number_of_trials, number_of_success). How would you sort that list of experiments based on their performance ? 

Sorting by n_success - n_failure has the risk of making popular experiments bubble up on top. For example, an experiment of 20 failures out of 100 trials will trump over some experiments with 8 successes out of 10.

Sorting by average rate, which is n_success/n_trials does not take into account the risk of having too little data on some experiments. That sorting scheme will rank experiments with 50 success out of 100 on the same bar with one with 1 success out of 2. The latter has very small sample size, make any estimation with it unreliable.


One solution proposed here uses the lower bound of the Wilson (symmetric) confidence interval, which says: given this trials data, what is , with 95% probability, the estimation larger than? We are trying to estimate the lower bound of the 95% confidence interval of the true average rate. 


Using the lower bound instead of the average estimate reduces the risk of overestimating some value that we don't have a lot of data about. The intuition is that if something we don't have a lot of data for, we shouldn't be very optimistic about it. 



![image](/images/absort.png )



Picture: 2 distributions, same mean, but one varies wilder. Which do you think has a better differentiating power ?



Here I took the [idea](https://www.evanmiller.org/how-not-to-sort-by-average-rating.html) from Mr. Evan Miller and improve the formula with Jeffrey prior to make Bayesian fan happy. 

Foolproof function that you (and I) can shamelessly copy and paste

```
import numpy as np 
from scipy.stats import beta 

def bernoulli_sort(trial, success, interval=0.95,prior='jeffrey'):
    """
        Implement Bernoulli sorting function to compare multiple A/B test experiments
        
        
        Args:
            trial: numpy 1d-array of int          
            success: numpy 1d-array of int
            interval: float, the lower bound of this symmetric confidence interval is used for sorting
            prior: 'jeffrey','haldane', or tuple that specify the Beta distribution
            
        Returns:
            numpy array of int: sorted index of the best
            
        Note:
            This function use Non informative prior (Jeffrey prior, in this case Beta(0.5,0.5))
            If one want to use other priors, please specify in the prior 
    
    """
    assert len(trial) == len(success), \
            'Your arrays need to be the same length'
    assert 0 < interval < 1, \
            'Confidence interval must be within range of (0,1)'
    assert sum((trial - success)>0) == len(trial), \
            'More successes than actual trials, check your data!'
    assert prior in ['jeffrey','haldane'] or type(prior) is tuple, \
            "Error in setting priors, please use the default if you dont know what you are doing!"
    
    if prior == 'jeffrey': prior = (0.5,0.5)
    elif prior == 'haldane': prior = (0,0)
    else:
        pass #Thou shall bear they own responsibility
        
    failure = trial - success
    
    lbound,ubound = beta.interval(interval, success + prior[0], failure + prior[1])
    return np.argsort(lbound)
```
